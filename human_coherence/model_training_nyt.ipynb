{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fWjNb37UUMhm",
    "outputId": "cb6ac0f2-bcaa-46f1-f72e-bbf424e322f5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# !pip install -U gensim\n",
    "# nltk.download('punkt')\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O9LxGSh8Ukmy",
    "outputId": "a35b30f5-3ad3-4d56-ee19-06bb9a8ef907"
   },
   "outputs": [],
   "source": [
    "#model training for new york times \n",
    "\n",
    "df = pd.read_csv('nyt.csv', sep=',')\n",
    "\n",
    "columns = ['pub_date', 'web_url', 'content']\n",
    "df = df[columns]\n",
    "\n",
    "df.drop_duplicates(subset =\"content\", inplace = True)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "print('After droping duplicate rows and unneccesary columns\\n Number of Columns: {}\\\n",
    "        , rows: {}'.format(df.shape[1], df.shape[0]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "id": "dARXAS5MUdJ4",
    "outputId": "4d6b260e-e754-4ea4-ce63-0335763ee6e5"
   },
   "outputs": [],
   "source": [
    "# initalizing the werdnet lemmatizer\n",
    "lm = WordNetLemmatizer()\n",
    "# creating a new column to store rows after processing\n",
    "df['post_processed'] = np.nan\n",
    "\n",
    "stopwords.extend(['advertisement', 'supported'])\n",
    "def processing(content):\n",
    "\n",
    "    content = content.split(' ')\n",
    "#     removing stopwords\n",
    "    content = [word.strip().lower() for word in content if word.lower() not in stopwords]\n",
    "#     removing these punctuations from tokens like it will convert the word mode? into mode\n",
    "    rx = re.compile('([&#.:?!-()])*')\n",
    "    content = [rx.sub('', word) for word in content]\n",
    "    \n",
    "    content = [word for word in content if len(word)>1 and word.isalpha()]\n",
    "#   stemming the words  \n",
    "    content = [lm.lemmatize(word) for word in content]\n",
    "\n",
    "    return ' '.join(content)\n",
    "\n",
    "for i in range(len(df)):\n",
    "    \n",
    "    if 'opinion' not in df.iloc[i,1]:\n",
    "        df.iloc[i,-1] = processing(df.iloc[i,2])\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "print('After Removing Opinion articles, rows are reduced to : ', df.shape[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W4NV16KOVExD",
    "outputId": "d90405f8-0d5c-4f5f-a2a5-e1af18e820ab"
   },
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "#decomposing sentences into tokens \n",
    "tokens = [sentence.split(' ') for sentence in df['post_processed'] ]\n",
    "\n",
    "# Create Dictionary\n",
    "dct = corpora.Dictionary(tokens)\n",
    "print('Unique words before filtering/after pre-processing', len(dct))\n",
    "# no_below= 30\n",
    "dct.filter_extremes(no_below= 30, no_above=0.25 )\n",
    "print('Unique words after filtering', len(dct))\n",
    "# Create Corpus\n",
    "corpus = [dct.doc2bow(token) for token in tokens]\n",
    "\n",
    "lda_model = gensim.models.LdaModel(corpus=corpus, id2word=dct, num_topics=7, \n",
    "                                           random_state=100, chunksize=128, passes=10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pj8ht8sjVi1j"
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "\n",
    "# temp_file = datapath(\"lda.model\")\n",
    "lda_model.save('lda.model')\n",
    "# Load a potentially pretrained model from disk.\n",
    "# lda_model1 = gensim.models.LdaModel.load('lda.model')\n",
    "dct.save('dictionary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "cemkoy_tool.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
