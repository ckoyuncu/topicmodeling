{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fWjNb37UUMhm",
    "outputId": "cb6ac0f2-bcaa-46f1-f72e-bbf424e322f5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# !pip install -U gensim\n",
    "# nltk.download('punkt')\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O9LxGSh8Ukmy",
    "outputId": "a35b30f5-3ad3-4d56-ee19-06bb9a8ef907"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For NYT Original Number of Columns: 23, rows: 19927\n",
      "After droping duplicate rows and unneccesary columns\n",
      " Number of Columns: 3        , rows: 19742\n"
     ]
    }
   ],
   "source": [
    "# Loading the datasets\n",
    "\n",
    "guard_df = pd.read_csv('guardian.csv', sep=',')\n",
    "\n",
    "columns = ['firstPublicationDate', 'webUrl', 'bodyText']\n",
    "guard_df = guard_df[columns]\n",
    "guard_df.drop_duplicates(subset =\"bodyText\", inplace = True)\n",
    "\n",
    "print(guard_df.shape)\n",
    "guard_df.dropna(inplace=True)\n",
    "\n",
    "guard_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "id": "dARXAS5MUdJ4",
    "outputId": "4d6b260e-e754-4ea4-ce63-0335763ee6e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Removing Opinion articles, rows are reduced to :  17228\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pub_date</th>\n",
       "      <th>web_url</th>\n",
       "      <th>content</th>\n",
       "      <th>post_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-03-01T16:58:17+0000</td>\n",
       "      <td>https://www.nytimes.com/2020/03/01/upshot/coro...</td>\n",
       "      <td>Advertisement Supported by A sick day? Remote ...</td>\n",
       "      <td>sick day remote work easy job day care center ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-03-01T18:10:26+0000</td>\n",
       "      <td>https://www.nytimes.com/2020/03/01/health/coro...</td>\n",
       "      <td>Advertisement Supported by Two cases detected ...</td>\n",
       "      <td>two case detected week apart washington state ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-03-01T04:04:45+0000</td>\n",
       "      <td>https://www.nytimes.com/2020/02/29/us/trump-co...</td>\n",
       "      <td>Advertisement Supported by President Trump fou...</td>\n",
       "      <td>president trump found veering message discussi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-03-01T12:00:08+0000</td>\n",
       "      <td>https://www.nytimes.com/2020/03/01/business/th...</td>\n",
       "      <td>Advertisement Supported by with interest By Da...</td>\n",
       "      <td>interest david going tax week said it maybe ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-03-01T08:00:08+0000</td>\n",
       "      <td>https://www.nytimes.com/2020/03/01/world/europ...</td>\n",
       "      <td>Advertisement Supported by Inside the outbreak...</td>\n",
       "      <td>inside outbreak british film crew shooting ita...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   pub_date  \\\n",
       "1  2020-03-01T16:58:17+0000   \n",
       "2  2020-03-01T18:10:26+0000   \n",
       "3  2020-03-01T04:04:45+0000   \n",
       "4  2020-03-01T12:00:08+0000   \n",
       "5  2020-03-01T08:00:08+0000   \n",
       "\n",
       "                                             web_url  \\\n",
       "1  https://www.nytimes.com/2020/03/01/upshot/coro...   \n",
       "2  https://www.nytimes.com/2020/03/01/health/coro...   \n",
       "3  https://www.nytimes.com/2020/02/29/us/trump-co...   \n",
       "4  https://www.nytimes.com/2020/03/01/business/th...   \n",
       "5  https://www.nytimes.com/2020/03/01/world/europ...   \n",
       "\n",
       "                                             content  \\\n",
       "1  Advertisement Supported by A sick day? Remote ...   \n",
       "2  Advertisement Supported by Two cases detected ...   \n",
       "3  Advertisement Supported by President Trump fou...   \n",
       "4  Advertisement Supported by with interest By Da...   \n",
       "5  Advertisement Supported by Inside the outbreak...   \n",
       "\n",
       "                                      post_processed  \n",
       "1  sick day remote work easy job day care center ...  \n",
       "2  two case detected week apart washington state ...  \n",
       "3  president trump found veering message discussi...  \n",
       "4  interest david going tax week said it maybe ac...  \n",
       "5  inside outbreak british film crew shooting ita...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pre-processing steps\n",
    "\n",
    "%timeit\n",
    "# initalizing the werdnet lemmatizer\n",
    "lm = WordNetLemmatizer()\n",
    "# creating a new column to store rows after processing\n",
    "guard_df['post_processed'] = np.nan\n",
    "\n",
    "stopwords.extend(['advertisement', 'supported'])\n",
    "def processing(content):\n",
    "\n",
    "    content = content.split(' ')\n",
    "#   removing stopwords\n",
    "    content = [word.strip().lower() for word in content if word.lower() not in stopwords]\n",
    "#   removing punctuations\n",
    "    rx = re.compile('([&#.:?!-()])*')\n",
    "    content = [rx.sub('', word) for word in content]\n",
    "    \n",
    "    content = [word for word in content if len(word)>1 and word.isalpha()]\n",
    "#   lemmatizing    \n",
    "    content = [lm.lemmatize(word) for word in content]\n",
    "\n",
    "    return ' '.join(content)\n",
    "\n",
    "# removing opinion articles\n",
    "for i in range(len(guard_df)):\n",
    "    if 'opinion' not in guard_df.iloc[i,1]:\n",
    "        guard_df.iloc[i,-1] = processing(guard_df.iloc[i,2]) \n",
    "\n",
    "\n",
    "guard_df.dropna(inplace=True)\n",
    "guard_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W4NV16KOVExD",
    "outputId": "d90405f8-0d5c-4f5f-a2a5-e1af18e820ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words before filtering/after pre-processing 97388\n",
      "Unique words after filtering 15357\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "#decomposing sentences into tokens \n",
    "tokens = [sentence.split(' ') for sentence in df['post_processed'] ]\n",
    "\n",
    "# Create Dictionary\n",
    "dct = corpora.Dictionary(tokens)\n",
    "print('Unique words before filtering/after pre-processing', len(dct))\n",
    "# no_below= 30\n",
    "dct.filter_extremes(no_below= 20, no_above=0.25 )\n",
    "print('Unique words after filtering', len(dct))\n",
    "# Create Corpus\n",
    "corpus = [dct.doc2bow(token) for token in tokens]\n",
    "\n",
    "lda_model = gensim.models.LdaModel(corpus=corpus, id2word=dct, num_topics=7, \n",
    "                                           random_state=100, chunksize=128, passes=10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "pj8ht8sjVi1j"
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "\n",
    "# temp_file = datapath(\"lda.model\")\n",
    "lda_model.save('lda.model')\n",
    "# Load a potentially pretrained model from disk.\n",
    "# lda_model1 = gensim.models.LdaModel.load('lda.model')\n",
    "dct.save('dictionary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "cemkoy_tool.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
